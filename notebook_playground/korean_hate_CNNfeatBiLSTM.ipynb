{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# 각종 전역변수들\n",
    "data_path = 'data\\\\korean-hate-speech\\\\labeled'\n",
    "path_train_data = 'data\\\\korean-hate-speech\\\\labeled\\\\train.tsv'\n",
    "path_dev_data = 'data\\\\korean-hate-speech\\\\labeled\\\\dev.tsv'\n",
    "BATCH_SIZE = 256\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LSTM_LAYER = 2\n",
    "n_classes = 3\n",
    "learning_rate = 0.01\n",
    "MAX_LEN = 80\n",
    "\n",
    "# 시드 고정\n",
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# CUDA setting 확인\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchtext dataset 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext를 이용한 전처리\n",
    "\n",
    "gensim word2vec_kor를 불러내서 torch에서 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_label(label, only_hate=False):\n",
    "    if only_hate is True:\n",
    "        label_matching = {'none':0, 'offensive':1, 'hate':1}\n",
    "    else:\n",
    "        label_matching = {'none':0, 'offensive':1, 'hate':2}\n",
    "    \n",
    "    return label_matching[label]\n",
    "\n",
    "def preprocess_bias(bias):\n",
    "    label_matching = {'none':0, 'others':1, 'gender':2}\n",
    "    \n",
    "    return label_matching[bias]\n",
    "\n",
    "def process_words(x):\n",
    "    words = []\n",
    "    for word in x:\n",
    "        char = []\n",
    "        for character in word:\n",
    "            char.append(character)\n",
    "        words.append(char)\n",
    "    #print(x, len(x), char, words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab('C:\\mecab\\mecab-ko-dic')\n",
    "\n",
    "# comments, hate 만 사용할 거임\n",
    "comments = data.Field(sequential=True,\n",
    "                      use_vocab=True,\n",
    "                      tokenize=tokenizer.morphs,\n",
    "                      lower=True,\n",
    "                      batch_first=True, \n",
    "                      fix_length = MAX_LEN)\n",
    "\n",
    "comments_char = data.Field(sequential=True,\n",
    "                           use_vocab=True,\n",
    "                           tokenize=tokenizer.morphs,\n",
    "                           lower=True,\n",
    "                           batch_first=True, \n",
    "                           fix_length = MAX_LEN,\n",
    "                           preprocessing=lambda x:process_words(x))\n",
    "\n",
    "contain_gender_bias = data.Field(sequential=False,\n",
    "                                 use_vocab=False,\n",
    "                                 batch_first=True, \n",
    "                                 preprocessing=lambda x: x =='True')\n",
    "\n",
    "bias = data.Field(sequential=False,\n",
    "                  use_vocab=False,\n",
    "                  batch_first=True, \n",
    "                  preprocessing=lambda x:preprocess_bias(x))\n",
    "\n",
    "hate = data.Field(sequential=False,\n",
    "                  use_vocab=False,\n",
    "                  is_target=True,\n",
    "                  batch_first=True, \n",
    "                  preprocessing=lambda x:preprocess_label(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 본격적인 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 6317\n",
      "검증 샘플의 개수 : 471\n",
      "테스트 샘플의 개수 : 1579\n",
      "{'comments': ['오창', '이채', '은', '이제', '그만', '좀', '나와라', '너무', '설정', '에', '집착', '진심', '도', '없', '구', '~', '너무', '애', '여우', '같', '아', '싫증'], 'contain_gender_bias': False, 'bias': 1, 'hate': 1, 'comments2': [['오', '창'], ['이', '채'], ['은'], ['이', '제'], ['그', '만'], ['좀'], ['나', '와', '라'], ['너', '무'], ['설', '정'], ['에'], ['집', '착'], ['진', '심'], ['도'], ['없'], ['구'], ['~'], ['너', '무'], ['애'], ['여', '우'], ['같'], ['아'], ['싫', '증']]}\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = TabularDataset.splits(\n",
    "    path=data_path, train='train2.tsv', validation='dev2.tsv', format='tsv',\n",
    "    fields=[('comments', comments), ('contain_gender_bias', contain_gender_bias), ('bias', bias), ('hate', hate), ('comments2', comments_char)],\n",
    "    skip_header=True)\n",
    "\n",
    "train_data, test_data = train_data.split(split_ratio=0.8)\n",
    "\n",
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
    "print('검증 샘플의 개수 : {}'.format(len(val_data)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))\n",
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 집합 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_word2vec_kor = './data/word2vec_kor/ko.bin'\n",
    "model_kor_word2vec = gensim.models.Word2Vec.load(path_word2vec_kor)\n",
    "gensim_to_torch_kor_word2vec = 'torch_kor_word2vec.wv'\n",
    "model_kor_word2vec.wv.save_word2vec_format(gensim_to_torch_kor_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 4033\n",
      "임베딩 벡터 크기: torch.Size([4033, 200])\n",
      "글자 집합의 크기 : 1195\n"
     ]
    }
   ],
   "source": [
    "vectors = Vectors(name=gensim_to_torch_kor_word2vec)\n",
    "comments.build_vocab(train_data, vectors=vectors, min_freq=3, max_size=10000)\n",
    "#hate.build_vocab(train_data)\n",
    "print('단어 집합의 크기 : {}'.format(len(comments.vocab)))\n",
    "print('임베딩 벡터 크기: {}'.format(comments.vocab.vectors.shape))\n",
    "\n",
    "comments_char.build_vocab(train_data, min_freq=3, max_size=10000)\n",
    "print('글자 집합의 크기 : {}'.format(len(comments_char.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로더 만들기\n",
    "\n",
    "본래 미니 배치 간 샘플 길이가 모두 다르지만, 앞에 torchtext 변수를 선언할때 fix_length를 이용해 통일시켜주었다. CNN에 집어넣어야하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 25\n",
      "테스트 데이터의 미니 배치의 개수 : 7\n",
      "검증 데이터의 미니 배치의 개수 : 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 다른 방법\n",
    "train_loader = Iterator(dataset=train_data, batch_size = BATCH_SIZE)\n",
    "test_loader = Iterator(dataset=test_data, batch_size = BATCH_SIZE)\n",
    "\n",
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))\n",
    "\"\"\"\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train_data, val_data, test_data), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False, sort_within_batch=False, sort_key=lambda x: len(x.comments))\n",
    "\n",
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN feature + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNfeatured_BiLSTM(nn.Module):\n",
    "    def __init__(self, pre_embedding,\n",
    "                 char_emb_dim, char_vocab_size,\n",
    "                 hidden_dim, model_embedding, num_lstm_layer, n_classes,\n",
    "                 dropout=0.3):\n",
    "        super(CNNfeatured_BiLSTM, self).__init__()\n",
    "        \n",
    "        #variable for cnn\n",
    "        channel_input_word = 1\n",
    "        channel_output = 32\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        # variable for bilstm\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_lstm_layer = num_lstm_layer\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        # char_cnn layer 세팅\n",
    "        self.char_emb = nn.Embedding(self.char_vocab_size,\n",
    "                                     self.char_emb_dim, padding_idx=0)\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "            #nn.Conv2d(channel_input_word, channel_output, kernel_size=(3, char_emb_dim), stride=1, padding=1),\n",
    "            nn.Conv2d(channel_input_word, channel_output, kernel_size=(3, char_emb_dim), stride=1),\n",
    "            nn.ReLU())\n",
    "        self.maxpool1d = nn.MaxPool1d(78)\n",
    "        \n",
    "        # embedding dimension 200\n",
    "        self.embedding = nn.Embedding.from_pretrained(pre_embedding, freeze=False)\n",
    "        # BiLSTM layer 세팅\n",
    "        self.bi_lstm = nn.LSTM(input_size=self.embedding.embedding_dim+self.char_emb_dim,\n",
    "                               hidden_size=self.hidden_dim,\n",
    "                               num_layers=self.num_lstm_layer,\n",
    "                               dropout=dropout,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=True)\n",
    "        \n",
    "        # last linear layer 세팅\n",
    "        # bidirectional 이라서 hidden_dim * 2\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, self.n_classes)\n",
    "                \n",
    "    def forward(self, sents, chars):\n",
    "        # sents:  torch.Size([128, 80]) = [batch_size, word_num_per_sentence]\n",
    "        print('sents: ', sents.shape)\n",
    "        print('chars: ', chars.shape)\n",
    "        # embedding\n",
    "        # word_embedded:  torch.Size([128, 80, 200]) = [batch_size, word_num_per_sentence, word_embedding_size]\n",
    "        word_embedded = self.embedding(sents)\n",
    "        print('word_embedded: ', word_embedded.shape)\n",
    "        \n",
    "        for i in range(chars.size(1)):\n",
    "            char_embedded = self.char_emb(sents[:, 1])\n",
    "            print('char_embedded: ', char_embedded.shape)\n",
    "        \"\"\"\n",
    "        char_embedded = self.char_emb(sents)\n",
    "        \n",
    "        char_embedded = char_embedded.unsqueeze(1)\n",
    "        print('unsqueezed char_embedded: ', char_embedded.shape)\n",
    "        char_embedded = self.cnn_layer(char_embedded)\n",
    "        print('after cnn_layer char_embedded: ', char_embedded.shape)\n",
    "        char_embedded = char_embedded.squeeze(3)\n",
    "        print('squeeze cnn_layer char_embedded: ', char_embedded.shape)\n",
    "        char_embedded = self.maxpool1d(char_embedded)\n",
    "        print('after maxpooling1d char_embedded: ', char_embedded.shape)\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = torch.cat([word_embedded, char_embedded], dim=-1)\n",
    "        print('bilstm input: ', embedded.shape)\n",
    "        \n",
    "        # bilstm\n",
    "        lstm_out, (h_n, c_n) = self.bi_lstm(embedded)\n",
    "        \n",
    "        # forward와 backward의 마지막 time-step의 은닉 상태를 가지고 와서 concat\n",
    "        # 이때 모델이 batch_first라는 점에 주의한다. (dimension 순서가 바뀜)\n",
    "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim = 1)\n",
    "        out=self.linear(hidden)\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    \n",
    "    corrects, total_loss = 0, 0\n",
    "    \n",
    "    for b, batch in enumerate(train_iter):\n",
    "        # comments 는 x, hate(label)은 y로 두고\n",
    "        x, y = (batch.comments.to(DEVICE), batch.comments_char.to(DEVICE)), batch.hate.to(DEVICE)\n",
    "        # gradient 0으로 세팅해두고\n",
    "        optimizer.zero_grad()\n",
    "        # model 돌리고\n",
    "        prediction = model(x)\n",
    "        # loss 구해서 backprop\n",
    "        loss = criterion(prediction, y)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    size = len(train_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    \n",
    "    return avg_loss\n",
    "        \n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    \n",
    "    corrects, total_loss = 0, 0\n",
    "    \n",
    "    for b, batch in enumerate(val_iter):\n",
    "        x, y = batch.comments.to(DEVICE), batch.hate.to(DEVICE)\n",
    "        prediction = model(x)\n",
    "        loss = criterion(prediction, y)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        corrects = corrects + (prediction.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        \n",
    "        y = y.data\n",
    "        y = y.to(\"cpu\")\n",
    "        y = y.detach().numpy()\n",
    "        p = prediction.max(1)[1].data\n",
    "        p = p.to(\"cpu\")\n",
    "        p = p.detach().numpy()\n",
    "    \n",
    "    print('**** Look only last batch case ****')\n",
    "    print(metrics.classification_report(y,p))\n",
    "        \n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    \n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN feature+BiLSTM 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNfeatured_BiLSTM(\n",
      "  (char_emb): Embedding(5000, 30, padding_idx=0)\n",
      "  (cnn_layer): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 30), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (maxpool1d): MaxPool1d(kernel_size=78, stride=78, padding=0, dilation=1, ceil_mode=False)\n",
      "  (embedding): Embedding(4033, 200)\n",
      "  (bi_lstm): LSTM(230, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "max_word_len = 10\n",
    "num_filters = 32\n",
    "char_emb_dim = 30\n",
    "char_vocab_size = 5000\n",
    "final_char_dim = 50\n",
    "cnn_feat_bilstm_model = CNNfeatured_BiLSTM(pre_embedding=comments.vocab.vectors,\n",
    "                                           char_emb_dim=char_emb_dim,\n",
    "                                           char_vocab_size=char_vocab_size,\n",
    "                                           hidden_dim=HIDDEN_DIM,\n",
    "                                           model_embedding=model_kor_word2vec,\n",
    "                                           num_lstm_layer=NUM_LSTM_LAYER,\n",
    "                                           n_classes=n_classes)\n",
    "\n",
    "print(cnn_feat_bilstm_model)\n",
    "cnn_feat_bilstm_model = cnn_feat_bilstm_model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(cnn_feat_bilstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN feature+BiLSTM training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STAGE] Train\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-655ca2b1c2e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[STAGE] Train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_feat_bilstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_feat_bilstm_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     result = (\n",
      "\u001b[1;32m<ipython-input-10-e36e236153ca>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_iter)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcorrects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;31m# comments 는 x, hate(label)은 y로 두고\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments_char\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchtext\\data\\iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchtext\\data\\batch.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, dataset, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                     \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, batch, device)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[1;34m(self, arr, device)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torchtext\\data\\field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "num_epoch = 10\n",
    "\n",
    "print('[STAGE] Train')\n",
    "for i in range(1, num_epoch+1):\n",
    "    train_loss = train(cnn_feat_bilstm_model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(cnn_feat_bilstm_model, val_iter)\n",
    "    result = (\n",
    "        f'[Epoch: {i}/{num_epoch}] train loss : {train_loss:.4f} '\n",
    "        f'| val loss : {val_loss:.4f} | val accuracy : {val_accuracy:.4f}%'\n",
    "    )\n",
    "    print(result)\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(cnn_bilstm_model.state_dict(), './snapshot/hate_classification_cnn_feat_bilstm.pt')\n",
    "        best_val_loss = val_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_bilstm_model.load_state_dict(torch.load('./snapshot/hate_classification_cnn+bilstm.pt'))\n",
    "test_loss, test_acc = evaluate(cnn_bilstm_model, test_iter)\n",
    "result = f'테스트 오차: {test_loss:.4f} | 테스트 정확도: {test_acc:.4f}%'\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "----------------------------------------\n",
    "----------------------------------------\n",
    "----------------------------------------\n",
    "----------------------------------------\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 max_word_len=10,\n",
    "                 num_filters=32,\n",
    "                 char_vocab_size=1000,\n",
    "                 char_emb_dim=30,\n",
    "                 final_char_dim=50,\n",
    "                 dropout=0.25,\n",
    "                 n_hidden_linear=100):\n",
    "        super(CharCNN, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('x : ', x.shape)\n",
    "        #batch_size = x.size(0)\n",
    "        #max_seq_len = x.size(1)\n",
    "        #max_word_len = x.size(2)\n",
    "\n",
    "        x = self.char_emb(x)  # (b, s, w, d)\n",
    "        x = x.view(batch_size * max_seq_len, max_word_len, -1)  # (b*s, w, d)\n",
    "        x = x.transpose(2, 1)  # (b*s, d, w): Conv1d takes in (batch, dim, seq_len), but raw embedded is (batch, seq_len, dim)\n",
    "\n",
    "        conv_lst = [conv(x) for conv in self.convs]\n",
    "        conv_concat = torch.cat(conv_lst, dim=-1)  # (b*s, num_filters, len(kernel_lst))\n",
    "        conv_concat = conv_concat.view(conv_concat.size(0), -1)  # (b*s, num_filters * len(kernel_lst))\n",
    "\n",
    "        output = self.linear(conv_concat)  # (b*s, final_char_dim)\n",
    "        output = output.view(batch_size, max_seq_len, -1)  # (b, s, final_char_dim)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
