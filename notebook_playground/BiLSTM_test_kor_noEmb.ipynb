{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1377c98b190>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각종 전역변수들\n",
    "data_path = 'my_korean_hate_speech\\\\data\\\\naver_review'\n",
    "path_train_data = 'my_korean_hate_speech\\\\data\\\\naver_review\\\\ratings_train.tsv'\n",
    "path_dev_data = 'my_korean_hate_speech\\\\data\\\\naver_review\\\\ratings_test.tsv'\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LSTM_LAYER = 2\n",
    "n_classes = 2\n",
    "learning_rate = 0.01\n",
    "MAX_LEN = 120\n",
    "\n",
    "# 시드 고정\n",
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA setting 확인\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (150000, 3)\n",
      "dev data shape: (50000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data read\n",
    "train_data = pd.read_csv(path_train_data, sep='\\t')\n",
    "dev_data = pd.read_csv(path_dev_data, sep='\\t')\n",
    "\n",
    "print(\"train data shape: {}\".format(train_data.shape))\n",
    "print(\"dev data shape: {}\".format(dev_data.shape))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_label(label, only_hate=False):\n",
    "    label_matching = {'0':0, '1':1}\n",
    "    \n",
    "    return label_matching[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab('C:\\mecab\\mecab-ko-dic')\n",
    "\n",
    "id = data.Field(sequential=False,\n",
    "                  use_vocab=False)\n",
    "\n",
    "document = data.Field(sequential=True,\n",
    "                      use_vocab=True,\n",
    "                      tokenize=tokenizer.morphs,\n",
    "                      lower=True,\n",
    "                      batch_first=True, \n",
    "                      fix_length = MAX_LEN)\n",
    "\n",
    "label = data.Field(sequential=False,\n",
    "                   use_vocab=False,\n",
    "                   is_target=True,\n",
    "                   batch_first=True, \n",
    "                   preprocessing=lambda x:preprocess_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 120000\n",
      "검증 샘플의 개수 : 30000\n",
      "테스트 샘플의 개수 : 50000\n",
      "{'id': '1894102', 'document': ['ㅋㅋㅋ', 'ㅋㅋㅋ', 'ㅋㅋㅋ', 'ㅋㅋㅋ', 'ㅋㅋㅋ', 'ㅋㅋㅋ', 'ㅋㅋㅋ', '멋있', '는', '영화'], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = TabularDataset.splits(\n",
    "    path=data_path, train='ratings_train.tsv', test='ratings_test.tsv', format='tsv',\n",
    "    fields=[('id', id), ('document', document), ('label', label)],\n",
    "    skip_header=True)\n",
    "\n",
    "train_data, val_data = train_data.split(split_ratio=0.8)\n",
    "\n",
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
    "print('검증 샘플의 개수 : {}'.format(len(val_data)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))\n",
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in range(len(train_data)):\n",
    "    if max_len < len(vars(train_data[i])['document']):\n",
    "        max_len = len(vars(train_data[i])['document'])\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 20042\n"
     ]
    }
   ],
   "source": [
    "document.build_vocab(train_data, min_freq=3, max_size=30000)\n",
    "#hate.build_vocab(train_data)\n",
    "document.vocab.stoi\n",
    "\n",
    "print('단어 집합의 크기 : {}'.format(len(document.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 3750\n",
      "테스트 데이터의 미니 배치의 개수 : 1563\n",
      "검증 데이터의 미니 배치의 개수 : 938\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 다른 방법\n",
    "train_loader = Iterator(dataset=train_data, batch_size = BATCH_SIZE)\n",
    "test_loader = Iterator(dataset=test_data, batch_size = BATCH_SIZE)\n",
    "\n",
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))\n",
    "\"\"\"\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train_data, val_data, test_data), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False, sort_within_batch=False, sort_key=lambda x: len(x.document))\n",
    "\n",
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, pre_embedding, hidden_dim, num_lstm_layer, n_classes, dropout=0.1):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_lstm_layer = num_lstm_layer\n",
    "        self.n_classes = n_classes\n",
    "        self.embedding = nn.Embedding(20042, 200)\n",
    "        \n",
    "        # BiLSTM layer 세팅\n",
    "        self.bi_lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                               hidden_size=self.hidden_dim,\n",
    "                               num_layers=self.num_lstm_layer,\n",
    "                               dropout=dropout,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=True)\n",
    "        \n",
    "        # bidirectional 이라서 hidden_dim * 2\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, self.n_classes)\n",
    "        \n",
    "    def forward(self, sents):\n",
    "        # embedding \n",
    "        #print('sents: ', sents)\n",
    "        embedded = self.embedding(sents)\n",
    "        #print('embedded: ', embedded)\n",
    "        \n",
    "        # lstm 통과\n",
    "        lstm_out, (h_n, c_n) = self.bi_lstm(embedded) # (h_0, c_0) = (0, 0)\n",
    "        \n",
    "        # forward와 backward의 마지막 time-step의 은닉 상태를 가지고 와서 concat\n",
    "        # 이때 모델이 batch_first라는 점에 주의한다. (dimension 순서가 바뀜)\n",
    "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim = 1)\n",
    "        #print('hidden: ', hidden)\n",
    "        out=self.linear(hidden)\n",
    "        #print('out: ', out)\n",
    "        #out=self.lin_layers(hidden)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    \n",
    "    corrects, total_loss = 0, 0\n",
    "    \n",
    "    for b, batch in enumerate(train_iter):\n",
    "        # comments 는 x, hate(label)은 y로 두고\n",
    "        x, y = batch.document.to(DEVICE), batch.label.to(DEVICE)\n",
    "        # gradient 0으로 세팅해두고\n",
    "        optimizer.zero_grad()\n",
    "        # model 돌리고\n",
    "        prediction = model(x)\n",
    "        # loss 구해서 backprop\n",
    "        loss = criterion(prediction, y)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    size = len(train_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    \n",
    "    return avg_loss\n",
    "        \n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    \n",
    "    corrects, total_loss = 0, 0\n",
    "    \n",
    "    for b, batch in enumerate(val_iter):\n",
    "        x, y = batch.document.to(DEVICE), batch.label.to(DEVICE)\n",
    "        prediction = model(x)\n",
    "        loss = criterion(prediction, y)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        corrects = corrects + (prediction.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        \n",
    "        y = y.data\n",
    "        y = y.to(\"cpu\")\n",
    "        y = y.detach().numpy()\n",
    "        p = prediction.max(1)[1].data\n",
    "        p = p.to(\"cpu\")\n",
    "        p = p.detach().numpy()\n",
    "    \n",
    "    print('**** Look only last batch case ****')\n",
    "    print(metrics.classification_report(y,p))\n",
    "        \n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    \n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (embedding): Embedding(20042, 200)\n",
      "  (bi_lstm): LSTM(200, 128, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
      "  (linear): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bilstm_model = BiLSTM(document.vocab.vectors, HIDDEN_DIM, NUM_LSTM_LAYER, n_classes)\n",
    "print(bilstm_model)\n",
    "bilstm_model = bilstm_model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STAGE] Train\n",
      "**** Look only last batch case ****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        13\n",
      "           1       0.67      0.67      0.67         3\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.79      0.79      0.79        16\n",
      "weighted avg       0.88      0.88      0.88        16\n",
      "\n",
      "[Epoch: 1/5] train loss : 0.0124 | val loss : 0.0112 | val accuracy : 84.2933%\n",
      "**** Look only last batch case ****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.62      0.76        13\n",
      "           1       0.38      1.00      0.55         3\n",
      "\n",
      "    accuracy                           0.69        16\n",
      "   macro avg       0.69      0.81      0.65        16\n",
      "weighted avg       0.88      0.69      0.72        16\n",
      "\n",
      "[Epoch: 2/5] train loss : 0.0104 | val loss : 0.0113 | val accuracy : 84.2667%\n",
      "**** Look only last batch case ****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        13\n",
      "           1       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.94        16\n",
      "   macro avg       0.88      0.96      0.91        16\n",
      "weighted avg       0.95      0.94      0.94        16\n",
      "\n",
      "[Epoch: 3/5] train loss : 0.0101 | val loss : 0.0114 | val accuracy : 84.2733%\n",
      "**** Look only last batch case ****\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        13\n",
      "           1       0.67      0.67      0.67         3\n",
      "\n",
      "    accuracy                           0.88        16\n",
      "   macro avg       0.79      0.79      0.79        16\n",
      "weighted avg       0.88      0.88      0.88        16\n",
      "\n",
      "[Epoch: 4/5] train loss : 0.0098 | val loss : 0.0112 | val accuracy : 84.6767%\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "num_epoch = 5\n",
    "\n",
    "print('[STAGE] Train')\n",
    "for i in range(1, num_epoch+1):\n",
    "    train_loss = train(bilstm_model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(bilstm_model, val_iter)\n",
    "    result = (\n",
    "        f'[Epoch: {i}/{num_epoch}] train loss : {train_loss:.4f} '\n",
    "        f'| val loss : {val_loss:.4f} | val accuracy : {val_accuracy:.4f}%'\n",
    "    )\n",
    "    print(result)\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(bilstm_model.state_dict(), './snapshot/bilstm_test.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_model.load_state_dict(torch.load('./snapshot/bilstm_test.pt'))\n",
    "test_loss, test_acc = evaluate(bilstm_model, test_iter)\n",
    "result = f'테스트 오차: {test_loss:.4f} | 테스트 정확도: {test_acc:.4f}%'\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
