{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu와 cuda 중 다음 기기로 학습함: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext.data import Iterator\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# 각종 전역변수들\n",
    "data_path = 'data\\\\korean-hate-speech\\\\labeled'\n",
    "path_train_data = 'data\\\\korean-hate-speech\\\\labeled\\\\train.tsv'\n",
    "path_dev_data = 'data\\\\korean-hate-speech\\\\labeled\\\\dev.tsv'\n",
    "BATCH_SIZE = 256\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LSTM_LAYER = 2\n",
    "n_classes = 3\n",
    "learning_rate = 0.01\n",
    "MAX_LEN = 80\n",
    "\n",
    "# 시드 고정\n",
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# CUDA setting 확인\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchtext dataset 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고자료  \n",
    "https://wikidocs.net/65348  \n",
    "https://wikidocs.net/64904  \n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "elmo 임베딩\n",
    "https://github.com/HIT-SCIR/ELMoForManyLangs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext를 이용한 전처리\n",
    "\n",
    "gensim word2vec_kor를 불러내서 torch에서 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_label(label, only_hate=False):\n",
    "    if only_hate is True:\n",
    "        label_matching = {'none':0, 'offensive':1, 'hate':1}\n",
    "    else:\n",
    "        label_matching = {'none':0, 'offensive':1, 'hate':2}\n",
    "    \n",
    "    return label_matching[label]\n",
    "\n",
    "def preprocess_bias(bias):\n",
    "    label_matching = {'none':0, 'others':1, 'gender':2}\n",
    "    \n",
    "    return label_matching[bias]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab('C:\\mecab\\mecab-ko-dic')\n",
    "\n",
    "# comments, hate 만 사용할 거임\n",
    "comments = data.Field(sequential=True,\n",
    "                      use_vocab=True,\n",
    "                      tokenize=tokenizer.morphs,\n",
    "                      lower=True,\n",
    "                      batch_first=True, \n",
    "                      fix_length = MAX_LEN)\n",
    "\n",
    "contain_gender_bias = data.Field(sequential=False,\n",
    "                                 use_vocab=False,\n",
    "                                 batch_first=True, \n",
    "                                 preprocessing=lambda x: x =='True')\n",
    "\n",
    "bias = data.Field(sequential=False,\n",
    "                  use_vocab=False,\n",
    "                  batch_first=True, \n",
    "                  preprocessing=lambda x:preprocess_bias(x))\n",
    "\n",
    "hate = data.Field(sequential=False,\n",
    "                  use_vocab=False,\n",
    "                  is_target=True,\n",
    "                  batch_first=True, \n",
    "                  preprocessing=lambda x:preprocess_label(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 본격적인 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 개수 : 6317\n",
      "검증 샘플의 개수 : 471\n",
      "테스트 샘플의 개수 : 1579\n",
      "{'comments': ['오창', '이채', '은', '이제', '그만', '좀', '나와라', '너무', '설정', '에', '집착', '진심', '도', '없', '구', '~', '너무', '애', '여우', '같', '아', '싫증'], 'contain_gender_bias': False, 'bias': 1, 'hate': 1}\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = TabularDataset.splits(\n",
    "    path=data_path, train='train.tsv', validation='dev.tsv', format='tsv',\n",
    "    fields=[('comments', comments), ('contain_gender_bias', contain_gender_bias), ('bias', bias), ('hate', hate)],\n",
    "    skip_header=True)\n",
    "\n",
    "train_data, test_data = train_data.split(split_ratio=0.8)\n",
    "\n",
    "print('훈련 샘플의 개수 : {}'.format(len(train_data)))\n",
    "print('검증 샘플의 개수 : {}'.format(len(val_data)))\n",
    "print('테스트 샘플의 개수 : {}'.format(len(test_data)))\n",
    "print(vars(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 집합 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_word2vec_kor = './data/word2vec_kor/ko.bin'\n",
    "model_kor_word2vec = gensim.models.Word2Vec.load(path_word2vec_kor)\n",
    "gensim_to_torch_kor_word2vec = 'torch_kor_word2vec.wv'\n",
    "model_kor_word2vec.wv.save_word2vec_format(gensim_to_torch_kor_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectors = Vectors(name=gensim_to_torch_kor_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 4033\n",
      "임베딩 벡터 크기: torch.Size([4033, 200])\n"
     ]
    }
   ],
   "source": [
    "comments.build_vocab(train_data, vectors=vectors, min_freq=3, max_size=10000)\n",
    "#hate.build_vocab(train_data)\n",
    "\n",
    "print('단어 집합의 크기 : {}'.format(len(comments.vocab)))\n",
    "print('임베딩 벡터 크기: {}'.format(comments.vocab.vectors.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로더 만들기\n",
    "\n",
    "본래 미니 배치 간 샘플 길이가 모두 다르지만, 앞에 torchtext 변수를 선언할때 fix_length를 이용해 통일시켜주었다. CNN에 집어넣어야하기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 미니 배치의 개수 : 25\n",
      "테스트 데이터의 미니 배치의 개수 : 7\n",
      "검증 데이터의 미니 배치의 개수 : 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 다른 방법\n",
    "train_loader = Iterator(dataset=train_data, batch_size = BATCH_SIZE)\n",
    "test_loader = Iterator(dataset=test_data, batch_size = BATCH_SIZE)\n",
    "\n",
    "print('훈련 데이터의 미니 배치 수 : {}'.format(len(train_loader)))\n",
    "print('테스트 데이터의 미니 배치 수 : {}'.format(len(test_loader)))\n",
    "\"\"\"\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train_data, val_data, test_data), batch_size=BATCH_SIZE,\n",
    "        shuffle=True, repeat=False, sort_within_batch=False, sort_key=lambda x: len(x.comments))\n",
    "\n",
    "print('훈련 데이터의 미니 배치의 개수 : {}'.format(len(train_iter)))\n",
    "print('테스트 데이터의 미니 배치의 개수 : {}'.format(len(test_iter)))\n",
    "print('검증 데이터의 미니 배치의 개수 : {}'.format(len(val_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN + BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBiLSTM(nn.Module):\n",
    "    def __init__(self, pre_embedding, hidden_dim, model_embedding, num_lstm_layer, n_classes, dropout=0.3):\n",
    "        super(CNNBiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_lstm_layer = num_lstm_layer\n",
    "        self.n_classes = n_classes\n",
    "        # embedding dimension 200\n",
    "        self.embedding = nn.Embedding.from_pretrained(pre_embedding, freeze=False)\n",
    "        \n",
    "        # cnn layer 세팅\n",
    "        # kernel_sizes = [3, 3]   # kernel size는 홀수로 씀\n",
    "        # self.convs1 = [nn.Conv2d(1, embedding_dim, (k, embedding_dim), stride=1, padding=(k//2, 0)) for k in kernel_sizes]\n",
    "        self.cnn_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.embedding.embedding_dim, kernel_size=(3, self.embedding.embedding_dim), stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "        self.cnn_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(self.embedding.embedding_dim, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # BiLSTM layer 세팅\n",
    "        self.bi_lstm = nn.LSTM(input_size=3,\n",
    "                               hidden_size=self.hidden_dim,\n",
    "                               num_layers=self.num_lstm_layer,\n",
    "                               dropout=dropout,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=True)\n",
    "        \n",
    "        #self.maxpool1d = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # last linear layer 세팅\n",
    "        # bidirectional 이라서 hidden_dim * 2\n",
    "        self.linear = nn.Linear(self.hidden_dim * 2, self.n_classes)\n",
    "                \n",
    "        \n",
    "    def forward(self, sents):\n",
    "        # embedding\n",
    "        print('sents: ', sents.shape)\n",
    "        embedded = self.embedding(sents)\n",
    "        print('emebdded: ', embedded.shape)\n",
    "        \n",
    "        # conv2d에 넣기 위해 channel 차원을 늘려준다.\n",
    "        cnn_x = embedded.unsqueeze(1)\n",
    "        print('cnn_x: ', cnn_x.shape)\n",
    "        #print('unsqueeze cnn_x shape: {}'.format(cnn_x.shape))\n",
    "        # conv2d 통과\n",
    "        cnn_x = self.cnn_layer1(cnn_x)\n",
    "        print('cnn_x -> conv2d: ', cnn_x.shape)\n",
    "        #print('after cnn_layer 1 cnn_x shape: {}'.format(cnn_x.shape))\n",
    "        cnn_x = self.cnn_layer2(cnn_x)\n",
    "        print('cnn_x -> conv2d: ', cnn_x.shape)\n",
    "        #print('after cnn_layer 2 cnn_x shape: {}'.format(cnn_x.shape))\n",
    "        # lstm에 넣기 위해 channel 차원을 없애준다.\n",
    "        cnn_x = cnn_x.squeeze(1)\n",
    "        print('before lstm cnn_x: ', cnn_x.shape)\n",
    "        #print('after squeeze cnn_x shape: {}'.format(cnn_x.shape))\n",
    "        break\n",
    "\n",
    "        # lstm 통과\n",
    "        lstm_out, (h_n, c_n) = self.bi_lstm(cnn_x)\n",
    "        \n",
    "        # forward와 backward의 마지막 time-step의 은닉 상태를 가지고 와서 concat\n",
    "        # 이때 모델이 batch_first라는 점에 주의한다. (dimension 순서가 바뀜)\n",
    "        hidden = torch.cat((h_n[-2,:,:], h_n[-1,:,:]), dim = 1)\n",
    "        out=self.linear(hidden)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    \n",
    "    corrects, total_loss = 0, 0\n",
    "    \n",
    "    for b, batch in enumerate(train_iter):\n",
    "        # comments 는 x, hate(label)은 y로 두고\n",
    "        x, y = batch.comments.to(DEVICE), batch.hate.to(DEVICE)\n",
    "        # gradient 0으로 세팅해두고\n",
    "        optimizer.zero_grad()\n",
    "        # model 돌리고\n",
    "        prediction = model(x)\n",
    "        # loss 구해서 backprop\n",
    "        loss = criterion(prediction, y)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    size = len(train_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    \n",
    "    return avg_loss\n",
    "        \n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    \n",
    "    corrects, total_loss = 0, 0\n",
    "    \n",
    "    for b, batch in enumerate(val_iter):\n",
    "        x, y = batch.comments.to(DEVICE), batch.hate.to(DEVICE)\n",
    "        prediction = model(x)\n",
    "        loss = criterion(prediction, y)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        corrects = corrects + (prediction.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "        \n",
    "        y = y.data\n",
    "        y = y.to(\"cpu\")\n",
    "        y = y.detach().numpy()\n",
    "        p = prediction.max(1)[1].data\n",
    "        p = p.to(\"cpu\")\n",
    "        p = p.detach().numpy()\n",
    "    \n",
    "    print('**** Look only last batch case ****')\n",
    "    print(metrics.classification_report(y,p))\n",
    "        \n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    \n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+BiLSTM 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_bilstm_model = CNNBiLSTM(comments.vocab.vectors,\n",
    "                             HIDDEN_DIM, model_kor_word2vec,\n",
    "                             NUM_LSTM_LAYER, n_classes)\n",
    "print(cnn_bilstm_model)\n",
    "cnn_bilstm_model = cnn_bilstm_model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(cnn_bilstm_model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN+BiLSTM training / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "num_epoch = 10\n",
    "\n",
    "print('[STAGE] Train')\n",
    "for i in range(1, num_epoch+1):\n",
    "    train_loss = train(cnn_bilstm_model, optimizer, train_iter)\n",
    "    val_loss, val_accuracy = evaluate(cnn_bilstm_model, val_iter)\n",
    "    result = (\n",
    "        f'[Epoch: {i}/{num_epoch}] train loss : {train_loss:.4f} '\n",
    "        f'| val loss : {val_loss:.4f} | val accuracy : {val_accuracy:.4f}%'\n",
    "    )\n",
    "    print(result)\n",
    "    break\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(cnn_bilstm_model.state_dict(), './snapshot/hate_classification_cnn+bilstm.pt')\n",
    "        best_val_loss = val_loss\n",
    "        \n",
    "cnn_bilstm_model.load_state_dict(torch.load('./snapshot/hate_classification_cnn+bilstm.pt'))\n",
    "test_loss, test_acc = evaluate(cnn_bilstm_model, test_iter)\n",
    "result = f'테스트 오차: {test_loss:.4f} | 테스트 정확도: {test_acc:.4f}%'\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch]",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
